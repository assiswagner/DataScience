{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle DataSet: DataAnalyst - Job\n",
    "\n",
    "## Description\n",
    "### Abstract\n",
    "\n",
    "#### Looking for a job as Data Analyst? Maybe this dataset can help you.\n",
    "\n",
    "### About this dataset\n",
    "\n",
    "Amidst the pandemic many people lost their jobs, with this dataset it is possible to hone the job search so that more people in need can find employment.\n",
    "This dataset was created by picklesueat and contains more than 2000 job listing for data analyst positions, with features such as:\n",
    "\n",
    "    - Salary Estimate\n",
    "    - Location\n",
    "    - Company Rating\n",
    "    - Job Description\n",
    "\n",
    "#### How to use\n",
    "\n",
    "Find the best jobs by salary and company rating\n",
    "Explore skills required in job descriptions\n",
    "Predict salary based on industry, location, company revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents:\n",
    "\n",
    "    1. Python Libraries & Settings\n",
    "    2. Loading & Understanding the data\n",
    "        2.1 Inspecting the dataframe\n",
    "        2.2 Sumarizing data\n",
    "        2.3 Deleting unnecessary columns\n",
    "    3. Data Cleaning\n",
    "        3.1 Dealing with '-1' and 'Unknown' values.\n",
    "        3.2 Dealing with NaNs\n",
    "        3.3 Feature Engineering (estimate salary)\n",
    "        3.4 Reducing granularity\n",
    "    4. Overall DataViz\n",
    "    5. Questions/Hypotheses: Integrity\n",
    "    6. Feature Engineering: Job Descriptions\n",
    "    7. Perfumery: Cleanning data\n",
    "    8. Looking for Latitude and Logitude: GeoCoder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python Libraries & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters: display and graphics\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading & Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File DataAnalyst.csv does not exist: 'DataAnalyst.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-67357b9f8ba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataAnalyst.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File DataAnalyst.csv does not exist: 'DataAnalyst.csv'"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "\n",
    "data = pd.read_csv('DataAnalyst.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's quite troublesome working with collumns that contain capital letters and blank spaces. Let's change it:**\n",
    "\n",
    "    From:  Type of ownership\n",
    "    To: type_of_ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace(\" \" , \"_\")  \n",
    "data.columns = data.columns.str.lower()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecesary columns: unnamed_0\n",
    "data.drop(columns={'unnamed:_0'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumarizing data: using ProfileReport Lib\n",
    "\n",
    "def overview(df):\n",
    "    return ProfileReport(df, html={'style': {'full_width': True}}, sort=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overview(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets display categorical unique's values lesser than 15** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].nunique() <= 15:\n",
    "        print('')\n",
    "        print(column)\n",
    "        print('')\n",
    "        print('Uniques Values: ' + str(data[column].nunique()))\n",
    "        print(data[column].value_counts())\n",
    "        print('=-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleanning\n",
    "\n",
    "As we can see, many columns has '-1' value. Problably represents 'NaNs' values, except for 'easy_apply' columns, that is can represents 'False'.\n",
    "In the same hand, many columns has 'Unknown' values. If we don't know, it's the same as 'NaN'.\n",
    "\n",
    "Also, 'Revenue' has a great granularity, would be better change it for a small one, just like:\n",
    "\n",
    "        Less than $1 million (USD)\n",
    "    1 to $5 million (USD) + $5 to $10 million (USD)\n",
    "        10 to $25 million (USD) + $25 to $50 million (USD)\n",
    "    50 to $100 million (USD)\n",
    "\n",
    "Than, let's fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GENERAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Dealing with '-1' and 'Unknown' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['size'].replace('-1', 'Unknown', inplace=True)\n",
    "data['type_of_ownership'].replace('-1', 'Unknown', inplace=True)\n",
    "data['revenue'].replace('-1', 'Unknown / Non-Applicable', inplace=True)\n",
    "data['easy_apply'].replace('-1', False, inplace=True)\n",
    "\n",
    "data=data.replace(-1,np.nan)\n",
    "data=data.replace(-1.0,np.nan)\n",
    "data=data.replace('-1',np.nan)\n",
    "\n",
    "data['easy_apply'] = data['easy_apply'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Dealing with NaNs values\n",
    "\n",
    "Strategy:\n",
    "    \n",
    "    General --> Dop columns that contains more than 40% NaNs;\n",
    "    Company_name --> Only 1, let's drop it. \n",
    "    Founded e HeadQuarter --> Does not influences our analysis. Let's keep them.\n",
    "    Industry --> NaN Imputation: Let's look at companies that have the same size and revenue as que missing and and imput the most frequent (mode).\n",
    "    Rating --> We are using the mean.\n",
    "    Revenue, Sector, Size --> Mode\n",
    "    Salary_Estimate --> Just one value. Lets check what job is, then replace for the 'mode' for that job. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping more than 40% NaNs\n",
    "\n",
    "[data.drop(columns=[column], inplace=True) for column in data.columns if data[column].isnull().sum()/(data.shape[0])*100 > 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 SPECIFIC COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Industry - 15.67% de NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at 'size' and 'revenue' (Industry's NaNs)\n",
    "\n",
    "data[data['industry'].isnull()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry'].value_counts().head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_null = data[data['industry'].isnull()]\n",
    "ind_null.revenue.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing values has '1 to 50 employees' Siza en 'Unkown' revenue. Let's use it as a parameter to replace \n",
    "\n",
    "ind_count = data.loc[(data['size'] == '1 to 50 employees') & (data['revenue'] == 'Unknown / Non-Applicable')]\n",
    "ind_count.industry.value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['industry'].replace(np.nan, 'IT Services', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Rating - 12.07% NaNs: Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mean = data['rating'].mean()\n",
    "data['rating'].replace(np.nan, rating_mean, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Sector, Renevue and Size: Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moda_sector = data['sector'].mode()\n",
    "data['sector'].replace(np.nan, moda_sector[0], inplace = True)\n",
    "\n",
    "moda_revenue = data['revenue'].mode()\n",
    "data['revenue'].replace(np.nan, moda_revenue[0], inplace = True)\n",
    "\n",
    "moda_size = data['size'].mode()\n",
    "data['size'].replace(np.nan, moda_size[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Company_Name: Only 1, let's drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['company_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Salary_Estimate: Only 1. \n",
    "\n",
    "Let's check the job_title and then replace for the most frequent salary for that job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job_title'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['salary_estimate'].isnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a Management!! \n",
    "\n",
    "data_manag = data.loc[data['job_title'] == 'Data Management Analyst']\n",
    "data_manag['salary_estimate'].value_counts().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['salary_estimate'].replace(np.nan, '$41K-$86K (Glassdoor est.)', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Salary_Estimate: Feature Engineering\n",
    "\n",
    "That's a categorical feature, but a troublesome one.\n",
    "\n",
    "Some of binning ranges is too high, look at these ranges could be grouped:\n",
    "    \n",
    "    31k-59k\n",
    "    30k-54k\n",
    "    35k-42k\n",
    "\n",
    "But some of them are too high, look at these ranges:\n",
    "\n",
    "    31k-100k\n",
    "    49k-112k\n",
    "    72k-127k\n",
    "\n",
    "Then, let's just create Let's Create 3 columns (Min, Max e Mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "minn = []\n",
    "maxx = []\n",
    "\n",
    "data2.reset_index()\n",
    "for val in data2.salary_estimate:\n",
    "    valores = re.findall('[0-9]+', str(val))\n",
    "    if valores:\n",
    "        minn.append(str(valores[0]))\n",
    "        maxx.append(str(valores[1]))\n",
    "\n",
    "data2['min_salary'] = minn\n",
    "data2['max_salary'] = maxx\n",
    "\n",
    "\n",
    "data2[['min_salary', 'max_salary']] = data2[['min_salary', 'max_salary']].astype(int)\n",
    "\n",
    "\n",
    "media = []\n",
    "subdata = data2[['min_salary', 'max_salary']]\n",
    "for x in subdata:\n",
    "    operacao = (subdata.min_salary + subdata.max_salary)/2\n",
    "    media.append(operacao)\n",
    "data2['mean_salary'] = media[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[['min_salary', 'max_salary', 'mean_salary']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 REDUCING GRANULARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Type_of_ownership: Aggregation (reducing granularity)\n",
    "\n",
    "There is too many redundant values in that column. The biggest data values is classified as 'Private Company' and 'Public Company'. But when we check the values we can also see others 'private' business that can fit in the mains values, such as:\n",
    "\n",
    "    Company - Private                 1273\n",
    "    Company - Public                   452\n",
    "    NaN                                 163\n",
    "    Nonprofit Organization             124 (Private)\n",
    "    Subsidiary or Business Segment      89 (Private)\n",
    "    Government                          37 (Public)\n",
    "    College / University                34\n",
    "    Hospital                            19\n",
    "    Unknown                             16\n",
    "    Other Organization                  13 (Private)\n",
    "    Contract                            11 (Private)\n",
    "    Private Practice / Firm              9 (Private)\n",
    "    School / School District             9\n",
    "    Self-employed                        2 (Private)\n",
    "    Franchise                            2 (Private)\n",
    "\n",
    "As I'm not american I don't know how to classify Hospitals, Colleges/Universities and Schools. Probably many of them are publics and privates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type_of_ownership.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type_of_ownership.replace({'Nonprofit Organization':'Company - Private',\n",
    "                       'Subsidiary or Business Segment':'Company - Private',\n",
    "                       'Franchise': 'Company - Private',\n",
    "                        'Other Organization' : 'Company - Private',\n",
    "                        'Contract': 'Company - Private',\n",
    "                        'Self-employed':'Company - Private',\n",
    "                        'Private Practice / Firm': 'Company - Private',\n",
    "                        'Government':'Company - Public'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type_of_ownership.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.revenue.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.revenue.replace({'$1 to $5 million (USD)':'$1 to $10 million (USD)',\n",
    "                       '$5 to $10 million (USD)':'$1 to $10 million (USD)',\n",
    "                       '$10 to $25 million (USD)':'$10 to $50 million (USD)',\n",
    "                       '$25 to $50 million (USD)': '$10 to $50 million (USD)',\n",
    "                        }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.revenue.value_counts(1).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even reducing granulatiry, the ranges still high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Job_Title\n",
    "\n",
    "There are many synonyms in the Jobs. Let's rename the most representative ones. Look at the examples below:\n",
    "\n",
    "    =Data= Governance =Analyst=                     16\n",
    "    Lead =Data Analyst=                             15\n",
    "    =Data= Reporting =Analyst=                      13\n",
    "    Financial =Data Analyst=                        12\n",
    "    =Data Analyst= I                              11\n",
    "    =Data Analyst= III                            11\n",
    "    Marketing =Data Analyst=                       9\n",
    "    Sr =Data Analyst=                              9\n",
    "    =Data= Management =Analyst=                      8\n",
    "    Data Warehouse Analyst                       8\n",
    "    Data Science Analyst                         7\n",
    "    SQL Data Analyst                             7\n",
    "    Technical Data Analyst                       7\n",
    "    Research Data Analyst                        6\n",
    "    Healthcare Data Analyst                      6\n",
    "    Data Security Analyst                        6\n",
    "    Clinical Data Analyst                        6\n",
    "\n",
    "As we can see, the most 'Data Analyst' jobs has qualificators. We will group them.\n",
    "\n",
    "Let's have a special look at this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking TOP20 distributions in Job_Title\n",
    "\n",
    "top_20_job_titles = data2.job_title.value_counts().iloc[:20].index\n",
    "\n",
    "# Creating a DF to receive Top20\n",
    "df_top20_job_titles = data2[data2['job_title'].isin(top_20_job_titles)]\n",
    "\n",
    "#Plotting Top20 Salaryes\n",
    "\n",
    "sns.catplot(data=df_top20_job_titles, y='job_title', x='min_salary', kind='box', \n",
    "            height=10, aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.job_title.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look how many job_titles has 'senior' and 'management' in the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2['job_title'].str.contains('Senior').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2['job_title'].str.contains('Management')].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, there are too many 'managements' jobs using differents titles. Let's group them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_title(df, old, new):\n",
    "    \n",
    "    for job in df.job_title:\n",
    "        found = re.findall(old.lower(), job.lower())\n",
    "        if found: \n",
    "            df.job_title.replace(job, new, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_title(data2, 'management', 'Data Management')\n",
    "replace_title(data2, 'senior' or 'sr' or 'sr.', 'Senior Data Analyst')\n",
    "replace_title(data2, 'junior' or 'jr' or 'jr.', 'Junior Data Analyst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not 'senior', 'junior', 'management' and contains 'DATA ANALYST', replace for 'DATA ANALYST'.\n",
    "\n",
    "for job in data2.job_title:\n",
    "    found = re.findall('senior', job.lower()) or re.findall('junior', job.lower()) or re.findall('management', job.lower())\n",
    "    if not found:\n",
    "        outros = re.findall('data', job.lower()) or re.findall('analyst', job.lower())\n",
    "        if outros:\n",
    "            data2.job_title.replace(job, 'Data Analyst', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.job_title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overall DataViz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Job_Title\n",
    "\n",
    "sns.countplot('job_title', data=data2)\n",
    "plt.title('Job title distribution', fontsize=15)\n",
    "plt.xticks(rotation=45, fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data2, y='job_title', x='mean_salary', kind='box', \n",
    "            height=6, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min salaries per Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data2, y='job_title', x='min_salary', kind='box', \n",
    "            height=6, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4), dpi=100)\n",
    "plt.title('Min Salary')\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "sns.violinplot(x='job_title',y='min_salary', data=data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max salaries per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=data2, y='job_title', x='max_salary', kind='box', \n",
    "            height=6, aspect=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4), dpi=100)\n",
    "plt.title('Max Salary')\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "sns.violinplot(x='job_title',y='max_salary', data=data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Mean Salary per Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vamos observar a distribuição do Cargo x Média Salarial\n",
    "\n",
    "plt.figure(figsize=(8,5), dpi=80)\n",
    "sns.kdeplot(data2['mean_salary'].loc[data2['job_title']=='Junior Data Analyst'], shade=True, \n",
    "            color='purple', label='junior data Analyst', alpha=0.4)\n",
    "sns.kdeplot(data2['mean_salary'].loc[data2['job_title']=='Data Analyst'], shade=True, \n",
    "            color='red', label='Data Analyst', alpha=0.3)\n",
    "sns.kdeplot(data2['mean_salary'].loc[data2['job_title']=='Senior Data Analyst'], shade=True, \n",
    "            color='dodgerblue', label='Senior Data Analyst', alpha=0.6)\n",
    "sns.kdeplot(data2['mean_salary'].loc[data2['job_title']=='Data Management'], shade=True, \n",
    "            color='g', label='Management Data', alpha=0.4)\n",
    "plt.title('Distribuição por Cargo x Média Salárial')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top10 companies wih higher mean salaries\n",
    "\n",
    "data2.groupby('company_name')[['mean_salary']].mean().sort_values(['mean_salary'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gouping by higher RATING, COMPANY and MEAN SALARY\n",
    "\n",
    "data2.groupby('company_name')[['rating', 'mean_salary']].mean().sort_values(['rating'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing companies that has higher salaries and his ratings.\n",
    "\n",
    "data2.groupby('company_name')[['mean_salary', 'rating']].mean().sort_values(['mean_salary'],ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating\n",
    "\n",
    "plt.hist(data2['rating'])\n",
    "plt.title('Distribuição do Rating')\n",
    "plt.ylabel('Qtd de empresas')\n",
    "plt.xlabel('Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Questions/Hypotheses: Integrity\n",
    "\n",
    "    1. Is there any 'Junior' earning MORE than 80k per year?\n",
    "    2. Is there any 'Management' earning LESS than 50k per year?\n",
    "    3. Is there a correletion between 'rating' and 'mean_salary'?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2['mean_salary'] > 80.0].loc[data2['job_title']=='Junior Data Analyst'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2['mean_salary'] < 50.0].loc[data2['job_title']=='Data Management']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max and Min salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[['mean_salary', 'job_title']].loc[data2['mean_salary']== data2.mean_salary.min()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[['mean_salary', 'job_title']].loc[data2['mean_salary']== data2.mean_salary.max()].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, the minimum and maximum salaries belongs to 'Data Analyst', not Junior nor Managament. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any correlation between salary and rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[['rating', 'mean_salary']].corr()\n",
    "\n",
    "# Not that time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions: We can't trut fully in this dataset. We need to extract more than what we did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "So, we know that is something odd in Job Title/Salaries. The highest salaries belongs to Data Analyst and also the Lowest.\n",
    "\n",
    "Let's take a look at Job Description to find some clues.\n",
    "\n",
    "Let' try extract the 'Job Title' from 'Job Description'. We are looking in Job Descriptions the jobs that has Juniors, Senior or Managament and reclassify them, if its wringly classified in Job Title.\n",
    "\n",
    "Approuch:\n",
    "\n",
    "    1º Create a new DF thar contains all 'Senior' occurences in Job_Description;\n",
    "    2º We are looking for patterns and replace the job titles if it is necessary.\n",
    "    3º We will do this to 'Junior' and 'Management' as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data3 = data2.copy()\n",
    "data3.job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_senior = data3[data3['job_description'].str.contains('Senior' or 'senior' or 'Sr.' or 'sr.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_senior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3), dpi=100)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "sns.countplot('job_title', data=description_senior)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we can find the word 'Senior' in Data Analystar, Junior and also Management job titles.\n",
    "\n",
    "Let's go into 'Data Analyst' job Descriptions seeking for 'Senior'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seniors_ = description_senior.loc[description_senior['job_title'] == 'Data Analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seniors_.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see the fully job description of index1, that is classified as 'Data Analyst'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seniors_.job_description.to_list()[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bingo**\n",
    "\n",
    "As you can see, even classified as Data Analyst (Job Title), the description refers to a Senior Job: \n",
    "\n",
    "    (...) The Senior Business Consultant will be involved in the strategic planning of an engagement or helping the client \n",
    "     make decisions about their future IT direction (...)\n",
    "    (...)7-10 Years of relevant work experience requirede (...)\n",
    "\n",
    "**So, to have more accurace, we have to handle Job Description column and find the real Jobs Titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3), dpi=100)\n",
    "plt.xticks(fontsize=8)\n",
    "sns.countplot('min_salary', data=seniors_)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's just see the words close to 'Senior' word in  the 'Job Descriptions' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=re.compile('...........................senior..................................', flags=re.IGNORECASE)\n",
    "\n",
    "[re.findall(pattern, x) for x in seniors_.job_description]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeing the occurences, we can confirm that, for some positions, despite the Job Titles are classifed as  'Data Analyst', the jobs refers to 'Seniors Data Analyst'.\n",
    "\n",
    "For what we can see, the following words in 'Data Analyst' belongs to Senior positions:\n",
    "\n",
    "    Senior Business Consultant\n",
    "    Solutions Analyst - Senior\n",
    "    Senior Data Analyst\n",
    "    Senior Data EngineerAnalyst\n",
    "    Senior Data Systems\n",
    "    Senior Metadata Analyst\n",
    "    Senior Analyst/Analyst\n",
    "\n",
    "So, we can replace them all. Warning: In my opinion, It's NOT appropriate use NLTK's methods, once some positions has the 'Senior' word, but it's not a Senior positions such as:\n",
    "\n",
    "    supervision of the Senior\n",
    "    reports to the Senior\n",
    "    work directly with senior\n",
    "    etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data3.copy()\n",
    "\n",
    "count = 0\n",
    "senior = ['Senior Business Consultant', 'Solutions Analyst - Senior', 'Senior Data Analyst', 'Senior Data EngineerAnalyst', \n",
    "          'Senior Data Systems', 'Senior Metadata Analyst', 'Senior Analyst/Analyst']\n",
    "\n",
    "idx_to_replace = []\n",
    "\n",
    "\n",
    "for position in senior:\n",
    "    for idx, phrase in enumerate(data4.job_description):\n",
    "        found = re.findall(position, phrase)\n",
    "        if found:\n",
    "            idx_to_replace.append(idx)\n",
    "           \n",
    "for idx in idx_to_replace:\n",
    "    data4.job_title[idx] = 'Senior Data Analyst'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for JUNIOR\n",
    "\n",
    "description_jr = data4[data4['job_description'].str.contains('Junior' or 'junior' or 'Jr.' or 'jr.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_ = description_jr.loc[description_jr['job_title'] == 'Data Analyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vamos fazer uso das REGEX. Dentro da coluna 'Job_Description', vamos ver as palavras próximas a 'Senior'.\n",
    "\n",
    "pattern=re.compile('..................................Junior....................................' or \\\n",
    "                   '...............Jr..............', flags=re.IGNORECASE)\n",
    "    \n",
    "[re.findall(pattern, x) for x in junior_.job_description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_replace_jr = []\n",
    "\n",
    "for idx, phrase in enumerate(data4.job_description):\n",
    "    found = re.findall('The Junior Data Analyst will work closely', phrase)\n",
    "    found1 = re.findall('is available for a Junior Civil', phrase)\n",
    "    found2 = re.findall('recruiting for a Junior Compensation', phrase)\n",
    "    if found or found1 or found2:\n",
    "        idx_to_replace_jr.append(idx)\n",
    "\n",
    "for idx in idx_to_replace_jr:\n",
    "    data4.job_title[idx] = 'Junior Data Analyst'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.job_title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Perfumery: Cleanning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DROPPING:\n",
    "\n",
    "    job_description --> Extracted all we wanted\n",
    "    headquarter --> Small relevance\n",
    "    founded --> Small relevante, 29% missing data.\n",
    "    revenue --> Highest frequency os 35% and it's associated to 'Unknown'. The second higher has 10% frequency.\n",
    "    easy_apply --> More than 90% 'False'\n",
    "    industry --> High cardinality: 88 unique values. Let's use 'Sector' that has 24 unique values.\n",
    "\n",
    "CHANGING:\n",
    "\n",
    "    location --> Let's split the locations in States and Cities.\n",
    "    sector --> 24 unique values. Let's use the Top7 and rename the other to 'others'.\n",
    "\n",
    "KEEPING:\n",
    "\n",
    "    rating\n",
    "    company_name\n",
    "    job_title\n",
    "    salaries\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPING\n",
    "\n",
    "data5.drop(columns=['job_description', 'headquarters', 'founded', 'revenue', 'easy_apply', 'industry'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.sector.value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top7 Sectors\n",
    "\n",
    "top7_setores = data5.sector.value_counts()[0:6]\n",
    "top7_setores = top7_setores.index\n",
    "\n",
    "[data5['sector'].replace(value, 'Others', inplace=True) for value in data5.sector if value not in top7_setores]\n",
    "\n",
    "data5.sector.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before 'Sectors' transformation\n",
    "\n",
    "plt.figure(figsize=(12,3), dpi=100)\n",
    "plt.xticks(fontsize=4)\n",
    "sns.countplot('sector', data=data4)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after Sectors transformation \n",
    "plt.figure(figsize=(9,4), dpi=100)\n",
    "plt.xticks(fontsize=8)\n",
    "sns.countplot('sector', data=data5)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location\n",
    "\n",
    "data5.location.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma cidade por Estados\n",
    "\n",
    "data5['state'] = [value.split(',')[1].strip() for value in data5.location]\n",
    "data5['city'] = [value.split(',')[0].strip() for value in data5.location]\n",
    "\n",
    "data5[['city','state']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF is 'Arapahoe'?? GOOGLE Helps --> Colorado!! ¬¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5['state'].replace('Arapahoe', 'CO', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4), dpi=100)\n",
    "plt.xticks(fontsize=8)\n",
    "sns.countplot('state', data=data5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mininum Salaries per State\n",
    "\n",
    "plt.figure(figsize=(7,4), dpi=100)\n",
    "plt.title('Distribuição dos Salários MÍNIMOS por ESTADO')\n",
    "sns.boxplot(x='state',y='min_salary', data=data5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing top5 States\n",
    "\n",
    "top5_states = data5['state'].value_counts().iloc[0:5].index\n",
    "df_top5_states = data5[data5['state'].isin(top5_states)]\n",
    "\n",
    "plt.figure(figsize=(6,3), dpi=100)\n",
    "sns.countplot('state', data=df_top5_states)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Looking for Latitude and Logitude: GeoCoder\n",
    "\n",
    "Well... It's a dumb solution. Geocoder has acess limited, so I created a 3 lists (location, latitude and longitude) instead of a tuple's dictionary or or direct input in data5['lat'] / data5['lng'].\n",
    "\n",
    "Since I can't use GeoCoder for the nexts 24h, we have to work the (bad) tools we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencage.geocoder import OpenCageGeocode\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5['lat'] = data5['location']\n",
    "data5['lng'] = data5['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import settings\n",
    "\n",
    "key = geocoder_API_KEY\n",
    "geocoder = OpenCageGeocode(key)\n",
    "query = data5['location'][0]\n",
    "result = geocoder.geocode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = []\n",
    "lat = []\n",
    "lng = []\n",
    "\n",
    "for location in data5['location']:\n",
    "    if location not in place:\n",
    "        query = location\n",
    "        place.append(location)\n",
    "        result = geocoder.geocode(query)\n",
    "        lat.append(result[0]['geometry']['lat'])\n",
    "        lng.append(result[0]['geometry']['lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = {}\n",
    "for k, v in enumerate(place):\n",
    "    coord[v] = list(zip(lat, lng))[k]\n",
    "\n",
    "for place in data5.lat:\n",
    "    if place in coord.keys():\n",
    "        data5['lat'].replace(place, coord[place][0], inplace=True), \\\n",
    "        data5['lng'].replace(place, coord[place][1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>39.739236</td>\n",
       "      <td>-104.984862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>Centennial, CO</td>\n",
       "      <td>39.568064</td>\n",
       "      <td>-104.977831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>39.739236</td>\n",
       "      <td>-104.984862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>Centennial, CO</td>\n",
       "      <td>39.568064</td>\n",
       "      <td>-104.977831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>Broomfield, CO</td>\n",
       "      <td>39.920383</td>\n",
       "      <td>-105.069146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            location        lat         lng\n",
       "2248      Denver, CO  39.739236 -104.984862\n",
       "2249  Centennial, CO  39.568064 -104.977831\n",
       "2250      Denver, CO  39.739236 -104.984862\n",
       "2251  Centennial, CO  39.568064 -104.977831\n",
       "2252  Broomfield, CO  39.920383 -105.069146"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data5[['location', 'lat', 'lng']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Next steps: GeoPlotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bitc670ede472bd4380aa22d26ad251623b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
